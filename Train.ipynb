{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from youtube-8m utils.py\n",
    "def Dequantize(feat_vector, max_quantized_value=2, min_quantized_value=-2):\n",
    "  \"\"\"Dequantize the feature from the byte format to the float format.\n",
    "\n",
    "  Args:\n",
    "    feat_vector: the input 1-d vector.\n",
    "    max_quantized_value: the maximum of the quantized value.\n",
    "    min_quantized_value: the minimum of the quantized value.\n",
    "\n",
    "  Returns:\n",
    "    A float vector which has the same shape as feat_vector.\n",
    "  \"\"\"\n",
    "  assert max_quantized_value > min_quantized_value\n",
    "  quantized_range = max_quantized_value - min_quantized_value\n",
    "  scalar = quantized_range / 255.0\n",
    "  bias = (quantized_range / 512.0) + min_quantized_value\n",
    "  return feat_vector * scalar + bias\n",
    "\n",
    "def decode(feat_vector, feature_size):\n",
    "    return tf.reshape(tf.cast(tf.decode_raw(feat_vector, \n",
    "                                            tf.uint8), \n",
    "                              tf.float32),\n",
    "                      [-1, feature_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath is path to tfrecord\n",
    "# datatype is audio or video\n",
    "# output_features and output_labels are empty lists or existing lists\n",
    "def load_data(filepath, data_type, output_labels, output_features):\n",
    "    if data_type == 'audio':\n",
    "        context = {\n",
    "            'labels': tf.VarLenFeature(dtype=tf.int64)\n",
    "        }\n",
    "\n",
    "        feature_list = {\n",
    "            'audio_embedding': tf.FixedLenSequenceFeature([], dtype=tf.string)\n",
    "        }\n",
    "        feature_name = 'audio_embedding'\n",
    "        feature_len = 128\n",
    "\n",
    "    elif data_type == 'video':\n",
    "        context = {\n",
    "            'id': tf.FixedLenFeature([], dtype=tf.string),\n",
    "            'labels': tf.VarLenFeature(dtype=tf.int64)\n",
    "        }\n",
    "\n",
    "        feature_list = {\n",
    "            'rgb': tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "        }\n",
    "        feature_name = 'rgb'\n",
    "        feature_len = 128\n",
    "        \n",
    "        \n",
    "    tf.reset_default_graph()    \n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Read TFRecord file\n",
    "    reader = tf.TFRecordReader()\n",
    "    filename_queue = tf.train.string_input_producer([filepath])\n",
    "\n",
    "\n",
    "    # Extract features from serialized data\n",
    "\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    context, features = tf.io.parse_single_sequence_example(serialized_example,\n",
    "                                                    context_features=context,\n",
    "                                                    sequence_features=feature_list,\n",
    "                                                    example_name=None,\n",
    "                                                    name=None\n",
    "    )\n",
    "\n",
    "    # Many tf.train functions use tf.train.QueueRunner,\n",
    "    # so we need to start it before we read\n",
    "    tf.train.start_queue_runners(sess)\n",
    "    \n",
    "    \n",
    "    num_in_file = sum(1 for _ in tf.python_io.tf_record_iterator(filepath))\n",
    "\n",
    "    for i in range(num_in_file):\n",
    "        labels = context['labels'].eval()\n",
    "        label = labels.values[0]\n",
    "        data = Dequantize(decode(features[feature_name], feature_len)).eval()\n",
    "        output_labels.append(label)\n",
    "        output_features.append(data)\n",
    "\n",
    "    tf.InteractiveSession().close()\n",
    "    \n",
    "    return output_labels, output_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_output_labels = []\n",
    "audio_output_features = []\n",
    "audio_path = \"audio_1556754078.010573.tfrecord\"\n",
    "audio_output_labels, audio_output_features = load_data(audio_path,\n",
    "                                           'audio', audio_output_labels, audio_output_features)\n",
    "\n",
    "video_output_labels = []\n",
    "video_output_features = []\n",
    "video_path = \"video_1556754626.438139.tfrecord\"\n",
    "video_output_labels, video_output_features = load_data(video_path,\n",
    "                                           'video', video_output_labels, video_output_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_x shape:  (450, 10, 128)\n",
      "audio_y shape:  (450,)\n",
      "video_x shape:  (450, 10, 128)\n",
      "video_y shape:  (450,)\n"
     ]
    }
   ],
   "source": [
    "audio_x = np.array(audio_output_features)\n",
    "audio_y = np.array(audio_output_labels)\n",
    "video_x = np.array(video_output_features)\n",
    "video_y = np.array(video_output_labels)\n",
    "print('audio_x shape: ', audio_x.shape)\n",
    "print('audio_y shape: ', audio_y.shape)\n",
    "print('video_x shape: ', video_x.shape)\n",
    "print('video_y shape: ', video_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- At this point the desired data should be loaded --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(dataset, batch_size):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_model_1(features, labels, mode):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
