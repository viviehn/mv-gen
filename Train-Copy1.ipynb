{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from youtube-8m utils.py\n",
    "def Dequantize(feat_vector, max_quantized_value=2, min_quantized_value=-2):\n",
    "  \"\"\"Dequantize the feature from the byte format to the float format.\n",
    "\n",
    "  Args:\n",
    "    feat_vector: the input 1-d vector.\n",
    "    max_quantized_value: the maximum of the quantized value.\n",
    "    min_quantized_value: the minimum of the quantized value.\n",
    "\n",
    "  Returns:\n",
    "    A float vector which has the same shape as feat_vector.\n",
    "  \"\"\"\n",
    "  assert max_quantized_value > min_quantized_value\n",
    "  quantized_range = max_quantized_value - min_quantized_value\n",
    "  scalar = quantized_range / 255.0\n",
    "  bias = (quantized_range / 512.0) + min_quantized_value\n",
    "  return feat_vector * scalar + bias\n",
    "\n",
    "def decode(feat_vector, feature_size):\n",
    "    return tf.reshape(tf.cast(tf.decode_raw(feat_vector, \n",
    "                                            tf.uint8), \n",
    "                              tf.float32),\n",
    "                      [-1, feature_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath is path to tfrecord\n",
    "# datatype is audio or video\n",
    "# output_features and output_labels are empty lists or existing lists\n",
    "def load_data(filepath, data_type, output_labels, output_features):\n",
    "    if data_type == 'audio':\n",
    "        context = {\n",
    "            'labels': tf.VarLenFeature(dtype=tf.int64)\n",
    "        }\n",
    "\n",
    "        feature_list = {\n",
    "            'audio_embedding': tf.FixedLenSequenceFeature([], dtype=tf.string)\n",
    "        }\n",
    "        feature_name = 'audio_embedding'\n",
    "        feature_len = 128\n",
    "\n",
    "    elif data_type == 'video':\n",
    "        context = {\n",
    "            'id': tf.FixedLenFeature([], dtype=tf.string),\n",
    "            'labels': tf.VarLenFeature(dtype=tf.int64)\n",
    "        }\n",
    "\n",
    "        feature_list = {\n",
    "            'rgb': tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "        }\n",
    "        feature_name = 'rgb'\n",
    "        feature_len = 128\n",
    "        \n",
    "        \n",
    "    tf.reset_default_graph()    \n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Read TFRecord file\n",
    "    reader = tf.TFRecordReader()\n",
    "    filename_queue = tf.train.string_input_producer([filepath])\n",
    "\n",
    "\n",
    "    # Extract features from serialized data\n",
    "\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    context, features = tf.io.parse_single_sequence_example(serialized_example,\n",
    "                                                    context_features=context,\n",
    "                                                    sequence_features=feature_list,\n",
    "                                                    example_name=None,\n",
    "                                                    name=None\n",
    "    )\n",
    "\n",
    "    # Many tf.train functions use tf.train.QueueRunner,\n",
    "    # so we need to start it before we read\n",
    "    tf.train.start_queue_runners(sess)\n",
    "    \n",
    "    \n",
    "    num_in_file = sum(1 for _ in tf.python_io.tf_record_iterator(filepath))\n",
    "\n",
    "    for i in range(num_in_file):\n",
    "        labels = context['labels'].eval()\n",
    "        label = labels.values[0]\n",
    "        data = Dequantize(decode(features[feature_name], feature_len)).eval()\n",
    "        output_labels.append(label)\n",
    "        output_features.append(data)\n",
    "\n",
    "    tf.InteractiveSession().close()\n",
    "    \n",
    "    return output_labels, output_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivien/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "audio_output_labels = []\n",
    "audio_output_features = []\n",
    "audio_path = \"audio_1556745450.370243.tfrecord\"\n",
    "audio_output_labels, audio_output_features = load_data(audio_path,\n",
    "                                           'audio', audio_output_labels, audio_output_features)\n",
    "\n",
    "video_output_labels = []\n",
    "video_output_features = []\n",
    "video_path = \"video_1556754626.438139.tfrecord\"\n",
    "video_output_labels, video_output_features = load_data(video_path,\n",
    "                                           'video', video_output_labels, video_output_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_x shape:  (450, 10, 128)\n",
      "audio_y shape:  (450,)\n",
      "video_x shape:  (450, 10, 128)\n",
      "video_y shape:  (450,)\n"
     ]
    }
   ],
   "source": [
    "audio_x = np.array(audio_output_features)\n",
    "audio_y = np.array(audio_output_labels)\n",
    "video_x = np.array(video_output_features)\n",
    "video_y = np.array(video_output_labels)\n",
    "print('audio_x shape: ', audio_x.shape)\n",
    "print('audio_y shape: ', audio_y.shape)\n",
    "print('video_x shape: ', video_x.shape)\n",
    "print('video_y shape: ', video_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- At this point the desired data should be loaded --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(dataset_x, dataset_y, batch_size):\n",
    "    indices = list(np.random.randint(0, len(dataset_x), size=batch_size))\n",
    "    \n",
    "    # Recover what the entries for the batch are\n",
    "    batch_x = np.array([dataset_x[i] for i in indices])\n",
    "    batch_y = np.array([dataset_y[i] for i in indices])\n",
    "    \n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(embedded_batch_a, embedded_batch_b, a_labels, b_labels, margin):\n",
    "    y = tf.cast(tf.equal(a_labels, b_labels), tf.float32)\n",
    "    dist = tf.norm(embedded_batch_a - embedded_batch_b)\n",
    "    loss = y * .5 * tf.square(dist) + (1 - y) * .5 * tf.square(tf.maximum(0., margin - dist))\n",
    "    return tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(input_shape, labels_shape, embed_size=128, learning_rate=0.0001, l1_reg=0.0001, l2_reg=0.0001, margin=.05):\n",
    "    audio_inputs = tf.placeholder(shape=[None, input_shape[0], input_shape[1]], name=\"audio_inputs\", dtype=tf.float32)\n",
    "    video_inputs = tf.placeholder(shape=[None, input_shape[0], input_shape[1]], name=\"video_inputs\", dtype=tf.float32)\n",
    "    audio_labels = tf.placeholder(shape=[None], name=\"audio_labels\", dtype=tf.float32)\n",
    "    video_labels = tf.placeholder(shape=[None], name=\"video_labels\", dtype=tf.float32)\n",
    "\n",
    "    flattened_audio = tf.layers.flatten(audio_inputs)\n",
    "    flattened_video = tf.layers.flatten(video_inputs)\n",
    "    flattened_audio_labels = tf.layers.flatten(audio_labels)\n",
    "    flattened_video_labels = tf.layers.flatten(video_labels)\n",
    "\n",
    "    \n",
    "    #subnetwork = build_fc_net(inputs, embed_size, np.prod(video_input_shape), l1_reg, l2_reg)\n",
    "    weights = tf.trainable_variables()\n",
    "    if l1_reg != 0:\n",
    "        reg_1 = tf.contrib.layers.l1_regularizer(scale=l1_reg)\n",
    "    else:\n",
    "        reg_1 = 0\n",
    "    if l2_reg != 0:\n",
    "        reg_2 = tf.contrib.layers.l2_regularizer(scale=l2_reg)\n",
    "    else:\n",
    "        reg_2 = 0\n",
    "    #loss = contrastiveloss(audio_embed, video_embed)\n",
    "    # TODO: This really should be a contrastive loss i think?????\n",
    "    audio_embed = build_fc_net(flattened_audio, embed_size, l1_reg, l2_reg)\n",
    "    video_embed = build_fc_net(flattened_video, embed_size, l1_reg, l2_reg)\n",
    "    weights = tf.trainable_variables()\n",
    "\n",
    "    #mse = tf.losses.mean_squared_error(audio_embed, video_embed) # only use loss if labels don't match?\n",
    "    error = contrastive_loss(audio_embed, video_embed, flattened_audio_labels, flattened_video_labels, margin)\n",
    "    reg_penalty = tf.contrib.layers.apply_regularization(reg_1, weights) + tf.contrib.layers.apply_regularization(reg_2, weights)\n",
    "    loss = error + reg_penalty\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    return audio_inputs, audio_labels, video_inputs, video_labels, loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio input is a tf placeholder for the input audio features\n",
    "#video labels is a tf placeholder for the input video features\n",
    "#encode size is the desired size of the encoded vector (should be the same size as the video features)\n",
    "#l1 and l2 reg are the amount of weight to put on l1 and l2 regularizers for the loss\n",
    "\n",
    "def build_fc_net(input_data, embed_size, l1_reg=0, l2_reg=0):\n",
    "    net = build_fc_layers(input_data, embed_size)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer should be the flattened inputs\n",
    "def build_fc_layers(input_layer, output_size):\n",
    "    with tf.name_scope(\"model\"):\n",
    "        with tf.variable_scope(\"dense0\", reuse=tf.AUTO_REUSE) as scope:\n",
    "            h1 = tf.layers.dense(inputs=input_layer, units=1024, activation=tf.nn.tanh)\n",
    "            d1 = tf.layers.dropout(inputs=h1, rate=0.4)\n",
    "        with tf.variable_scope(\"dense1\", reuse=tf.AUTO_REUSE) as scope:\n",
    "            h2 = tf.layers.dense(inputs=d1, units=1024, activation=tf.nn.tanh)\n",
    "            d2 = tf.layers.dropout(inputs=h2, rate=0.4)            \n",
    "        with tf.variable_scope(\"dense2\", reuse=tf.AUTO_REUSE) as scope:\n",
    "            h3 = tf.layers.dense(inputs=d2, units=1024, activation=tf.nn.tanh)\n",
    "            d3 = tf.layers.dropout(inputs=h3, rate=0.4)\n",
    "        with tf.variable_scope(\"dense3\", reuse=tf.AUTO_REUSE) as scope:\n",
    "            raw_encode = tf.layers.dense(inputs=d3, units=output_size, activation=tf.nn.tanh)\n",
    "\n",
    "    return raw_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loss, optimizer, audio_x, audio_y, video_x, video_y, batch_size, num_iters=5000):\n",
    "    audio_inputs, audio_labels, video_inputs, video_labels, loss, optimizer = build_graph(audio_x[0].shape, audio_y.shape[0])\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for i in range(num_iters):\n",
    "            audio_batch_input, audio_batch_label = build_batch(audio_x, audio_y, batch_size)\n",
    "            video_batch_input, video_batch_label = build_batch(video_x, video_y, batch_size)\n",
    "            _, loss_val = sess.run([optimizer, loss], feed_dict={audio_inputs: audio_batch_input,\n",
    "                                                             video_inputs: video_batch_input,\n",
    "                                                             audio_labels: audio_batch_label,\n",
    "                                                             video_labels: video_batch_label})\n",
    "            if i % 100 == 0:\n",
    "                print(\"Loss at batch \" + str(i))\n",
    "                print(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at batch 0\n",
      "9.665541\n",
      "Loss at batch 100\n",
      "8.809877\n",
      "Loss at batch 200\n",
      "8.807909\n",
      "Loss at batch 300\n",
      "8.80874\n",
      "Loss at batch 400\n",
      "8.808024\n",
      "Loss at batch 500\n",
      "8.807798\n",
      "Loss at batch 600\n",
      "8.80401\n",
      "Loss at batch 700\n",
      "8.798994\n",
      "Loss at batch 800\n",
      "8.79342\n",
      "Loss at batch 900\n",
      "8.787271\n",
      "Loss at batch 1000\n",
      "8.780493\n",
      "Loss at batch 1100\n",
      "8.77808\n",
      "Loss at batch 1200\n",
      "8.779077\n",
      "Loss at batch 1300\n",
      "8.771797\n",
      "Loss at batch 1400\n",
      "8.768798\n",
      "Loss at batch 1500\n",
      "8.760667\n",
      "Loss at batch 1600\n",
      "8.7520075\n",
      "Loss at batch 1700\n",
      "8.972441\n",
      "Loss at batch 1800\n",
      "8.733269\n",
      "Loss at batch 1900\n",
      "8.723491\n",
      "Loss at batch 2000\n",
      "8.713846\n",
      "Loss at batch 2100\n",
      "8.702354\n",
      "Loss at batch 2200\n",
      "8.690161\n",
      "Loss at batch 2300\n",
      "8.677269\n",
      "Loss at batch 2400\n",
      "8.663697\n",
      "Loss at batch 2500\n",
      "8.649381\n",
      "Loss at batch 2600\n",
      "8.634296\n",
      "Loss at batch 2700\n",
      "8.625087\n",
      "Loss at batch 2800\n",
      "8.608751\n",
      "Loss at batch 2900\n",
      "8.59144\n",
      "Loss at batch 3000\n",
      "8.573368\n",
      "Loss at batch 3100\n",
      "8.554292\n",
      "Loss at batch 3200\n",
      "8.53428\n",
      "Loss at batch 3300\n",
      "8.5132885\n",
      "Loss at batch 3400\n",
      "8.491317\n",
      "Loss at batch 3500\n",
      "8.468199\n",
      "Loss at batch 3600\n",
      "8.444035\n",
      "Loss at batch 3700\n",
      "8.418856\n",
      "Loss at batch 3800\n",
      "8.392069\n",
      "Loss at batch 3900\n",
      "8.364084\n",
      "Loss at batch 4000\n",
      "8.334742\n",
      "Loss at batch 4100\n",
      "8.304021\n",
      "Loss at batch 4200\n",
      "8.271799\n",
      "Loss at batch 4300\n",
      "8.238027\n",
      "Loss at batch 4400\n",
      "8.202637\n",
      "Loss at batch 4500\n",
      "8.165614\n",
      "Loss at batch 4600\n",
      "8.1268\n",
      "Loss at batch 4700\n",
      "8.08613\n",
      "Loss at batch 4800\n",
      "8.04353\n",
      "Loss at batch 4900\n",
      "7.99895\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train(loss, optimizer, audio_x, audio_y, video_x, video_y, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input layer is a tf placeholder that is the input data\n",
    "#output size is the desired size of the encoded vector (should be the same size as the video vector?)\n",
    "#scope is a string to keep track of all trainable variables in this stack\n",
    "def build_fc_layers(input_layer, output_size):\n",
    "    h1 = tf.layers.dense(inputs=input_layer, units=1024, activation=tf.nn.tanh)\n",
    "    d1 = tf.layers.dropout(inputs=h1, rate=0.4)\n",
    "    h2 = tf.layers.dense(inputs=d1, units=1024, activation=tf.nn.tanh)\n",
    "    d2 = tf.layers.dropout(inputs=h2, rate=0.4)\n",
    "    h3 = tf.layers.dense(inputs=d2, units=1024, activation=tf.nn.tanh)\n",
    "    d3 = tf.layers.dropout(inputs=h3, rate=0.4)\n",
    "    raw_encode = tf.layers.dense(inputs=d3, units=output_size, activation=tf.nn.tanh)\n",
    "    return raw_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio input is a tf placeholder for the input audio features\n",
    "#video labels is a tf placeholder for the input video features\n",
    "#encode size is the desired size of the encoded vector (should be the same size as the video features)\n",
    "#l1 and l2 reg are the amount of weight to put on l1 and l2 regularizers for the loss\n",
    "def build_fc_net(audio_input, video_labels, encode_size, l1_reg=0, l2_reg=0):\n",
    "    audio_net = build_fc_layers(audio_input, encode_size)\n",
    "    weights = tf.trainable_variables()\n",
    "    if l1_reg != 0:\n",
    "        reg_1 = tf.contrib.layers.l1_regularizer(scale=l1_reg)\n",
    "    else:\n",
    "        reg_1 = 0\n",
    "    if l2_reg != 0:\n",
    "        reg_2 = tf.contrib.layers.l2_regularizer(scale=l2_reg)\n",
    "    else:\n",
    "        reg_2 = 0\n",
    "    mse = tf.losses.mean_squared_error(audio_net, video_labels)\n",
    "    reg_penalty = tf.contrib.layers.apply_regularization(reg_1, weights) + tf.contrib.layers.apply_regularization(reg_2, weights)\n",
    "    loss = mse + reg_penalty\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(audio_input_shape, video_input_shape, learning_rate=0.0001, l1_reg=0.0001, l2_reg=0):\n",
    "    inputs = tf.placeholder(shape=audio_input_shape, name=\"inputs\", dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=video_input_shape, name=\"labels\", dtype=tf.float32)\n",
    "    flattened_labels = tf.layers.Flatten()(labels)\n",
    "    loss = build_fc_net(inputs, flattened_labels, np.prod(video_input_shape), l1_reg, l2_reg)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    return inputs, labels, loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inputs, labels, loss, optimizer, dataset, batch_size, num_iters=1000):\n",
    "    with tf.Session() as sess:\n",
    "        for i in range(num_iters):\n",
    "            audio_batch_input, audio_batch_label = build_batch(dataset, batch_size)\n",
    "            _, loss = sess.run([optimizer, loss], feed_dict={inputs:batch_input, labels:batch_label})\n",
    "            if i % 100 == 0:\n",
    "                print(\"Loss at batch \" + str(i))\n",
    "                print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shapes (450, 1) and (450, 10, 450) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-44c7ad20257f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-40-ba52fa3cdafd>\u001b[0m in \u001b[0;36mbuild_graph\u001b[0;34m(audio_input_shape, video_input_shape, learning_rate, l1_reg, l2_reg)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvideo_input_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mflattened_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_fc_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_input_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-9755c1882c88>\u001b[0m in \u001b[0;36mbuild_fc_net\u001b[0;34m(audio_input, video_labels, encode_size, l1_reg, l2_reg)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mreg_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mreg_penalty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreg_penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(labels, predictions, weights, scope, loss_collection, reduction)\u001b[0m\n\u001b[1;32m    667\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m     \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquared_difference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m     return compute_weighted_loss(\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \"\"\"\n\u001b[1;32m   1022\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (450, 1) and (450, 10, 450) are incompatible"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
