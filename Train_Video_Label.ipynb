{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from youtube-8m utils.py\n",
    "def Dequantize(feat_vector, max_quantized_value=2, min_quantized_value=-2):\n",
    "  \"\"\"Dequantize the feature from the byte format to the float format.\n",
    "\n",
    "  Args:\n",
    "    feat_vector: the input 1-d vector.\n",
    "    max_quantized_value: the maximum of the quantized value.\n",
    "    min_quantized_value: the minimum of the quantized value.\n",
    "\n",
    "  Returns:\n",
    "    A float vector which has the same shape as feat_vector.\n",
    "  \"\"\"\n",
    "  assert max_quantized_value > min_quantized_value\n",
    "  quantized_range = max_quantized_value - min_quantized_value\n",
    "  scalar = quantized_range / 255.0\n",
    "  bias = (quantized_range / 512.0) + min_quantized_value\n",
    "  return feat_vector * scalar + bias\n",
    "\n",
    "def decode(feat_vector, feature_size):\n",
    "    return tf.reshape(tf.cast(tf.decode_raw(feat_vector, \n",
    "                                            tf.uint8), \n",
    "                              tf.float32),\n",
    "                      [-1, feature_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath is path to tfrecord\n",
    "# datatype is audio or video\n",
    "# output_features and output_labels are empty lists or existing lists\n",
    "def load_data(filepath, data_type, output_labels, output_features):\n",
    "    if data_type == 'audio':\n",
    "        context = {\n",
    "            'labels': tf.VarLenFeature(dtype=tf.int64)\n",
    "        }\n",
    "\n",
    "        feature_list = {\n",
    "            'audio_embedding': tf.FixedLenSequenceFeature([], dtype=tf.string)\n",
    "        }\n",
    "        feature_name = 'audio_embedding'\n",
    "        feature_len = 128\n",
    "\n",
    "    elif data_type == 'video':\n",
    "        context = {\n",
    "            'id': tf.FixedLenFeature([], dtype=tf.string),\n",
    "            'labels': tf.VarLenFeature(dtype=tf.int64)\n",
    "        }\n",
    "\n",
    "        feature_list = {\n",
    "            'rgb': tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "        }\n",
    "        feature_name = 'rgb'\n",
    "        feature_len = 128\n",
    "        \n",
    "        \n",
    "    tf.reset_default_graph()    \n",
    "\n",
    "\n",
    "    # Read TFRecord file\n",
    "    reader = tf.TFRecordReader()\n",
    "    filename_queue = tf.train.string_input_producer([filepath])\n",
    "\n",
    "\n",
    "    # Extract features from serialized data\n",
    "\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    context, features = tf.io.parse_single_sequence_example(serialized_example,\n",
    "                                                    context_features=context,\n",
    "                                                    sequence_features=feature_list,\n",
    "                                                    example_name=None,\n",
    "                                                    name=None\n",
    "    )\n",
    "    labels = context['labels']\n",
    "    label = labels.values[0]\n",
    "    data = Dequantize(decode(features[feature_name], feature_len))\n",
    "\n",
    "    # Many tf.train functions use tf.train.QueueRunner,\n",
    "    # so we need to start it before we read\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        # f = codecs.open(outpath, \"w\", encoding='utf-8')\n",
    "        try:\n",
    "            counter = 0\n",
    "            recordlist = []\n",
    "\n",
    "            num_in_file = sum(1 for _ in tf.python_io.tf_record_iterator(filepath))\n",
    "\n",
    "            for i in range(num_in_file):\n",
    "                d, l = sess.run([data, label])\n",
    "                output_labels.append(l)\n",
    "                output_features.append(d)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Finished extracting from tfrecord data.')\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "\n",
    "\n",
    "    \n",
    "    return output_labels, output_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_output_labels = []\n",
    "audio_output_features = []\n",
    "audio_path = \"audio_1556954623.538376.tfrecord\"\n",
    "audio_output_labels, audio_output_features = load_data(audio_path,\n",
    "                                           'audio', audio_output_labels, audio_output_features)\n",
    "# audio_path = \"audio_1556932956.096088.tfrecord\"\n",
    "# audio_output_labels, audio_output_features = load_data(audio_path,\n",
    "#                                            'audio', audio_output_labels, audio_output_features)\n",
    "\n",
    "video_output_labels = []\n",
    "video_output_features = []\n",
    "video_path = \"video_1556949434.26188.tfrecord\"\n",
    "video_output_labels, video_output_features = load_data(video_path,\n",
    "                                           'video', video_output_labels, video_output_features)\n",
    "# video_path = \"video_1556931967.658074.tfrecord\"\n",
    "# video_output_labels, video_output_features = load_data(video_path,\n",
    "#                                            'video', video_output_labels, video_output_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_x shape:  (1939, 10, 128)\n",
      "audio_y shape:  (1939,)\n",
      "video_x shape:  (1939, 10, 128)\n",
      "video_y shape:  (1939,)\n"
     ]
    }
   ],
   "source": [
    "audio_x = np.array(audio_output_features)\n",
    "audio_y = np.array(audio_output_labels)\n",
    "video_x = np.array(video_output_features)\n",
    "video_y = np.array(video_output_labels)\n",
    "print('audio_x shape: ', audio_x.shape)\n",
    "print('audio_y shape: ', audio_y.shape)\n",
    "print('video_x shape: ', video_x.shape)\n",
    "print('video_y shape: ', video_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(audio_y)\n",
    "print(video_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1552, 10, 128)\n",
      "(1552,)\n",
      "(1552, 10, 128)\n",
      "(1552,)\n"
     ]
    }
   ],
   "source": [
    "validation_split = int(audio_y.size * 0.2)\n",
    "audio_indices_val = np.where(audio_y < validation_split)\n",
    "video_indices_val = np.where(video_y < validation_split)\n",
    "audio_indices_train = np.where(audio_y >= validation_split)\n",
    "video_indices_train = np.where(video_y >= validation_split)\n",
    "validation_audio_x = audio_x[audio_indices_val]\n",
    "validation_audio_y = audio_y[audio_indices_val]\n",
    "validation_video_x = video_x[video_indices_val]\n",
    "validation_video_y = video_y[video_indices_val]\n",
    "training_audio_x = audio_x[audio_indices_train]\n",
    "training_audio_y = audio_y[audio_indices_train]\n",
    "training_video_x = video_x[video_indices_train]\n",
    "training_video_y = video_y[video_indices_train]\n",
    "\n",
    "validation_indices = np.random.choice(audio_y.size, int(audio_y.size*0.2), replace=False)\n",
    "validation_audio_x = audio_x[validation_indices]\n",
    "validation_audio_y = audio_y[validation_indices]\n",
    "validation_video_x = video_x[validation_indices]\n",
    "validation_video_y = video_y[validation_indices]\n",
    "\n",
    "training_audio_x = np.delete(audio_x, validation_indices, axis=0)\n",
    "training_audio_y = np.delete(audio_y, validation_indices, axis=0)\n",
    "training_video_x = np.delete(video_x, validation_indices, axis=0)\n",
    "training_video_y = np.delete(video_y, validation_indices, axis=0)\n",
    "\n",
    "print(training_audio_x.shape)\n",
    "print(training_audio_y.shape)\n",
    "print(training_video_x.shape)\n",
    "print(training_video_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 99, 99, 99])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- At this point the desired data should be loaded --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(dataset_x, dataset_y, batch_size, labels=None):\n",
    "    if labels is None:\n",
    "        indices = list(np.random.randint(0, len(dataset_x), size=batch_size))\n",
    "    else:\n",
    "        indices=[]\n",
    "        for i in labels:\n",
    "            #print(i)\n",
    "            indices.append(np.where(dataset_y == i)[0][0])\n",
    "    # Recover what the entries for the batch are\n",
    "    batch_x = np.array([dataset_x[i] for i in indices])\n",
    "    batch_y = np.array([dataset_y[i] for i in indices])\n",
    "    \n",
    "    return batch_x, batch_y, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_balanced_batch(dataset1_x, dataset1_y, dataset2_x, dataset2_y, batch_size):\n",
    "    b = int(batch_size/2)\n",
    "    indices = list(np.random.randint(0, len(dataset1_x), size=b))\n",
    "    labels = dataset1_y[indices]\n",
    "    matched1_x, matched1_y, _ = build_batch(dataset1_x, dataset1_y, b, labels)\n",
    "    matched2_x, matched2_y, _ = build_batch(dataset2_x, dataset2_y, b, labels)\n",
    "    random1_x, random1_y, _ = build_batch(dataset1_x, dataset1_y, b)    \n",
    "    random2_x, random2_y, _ = build_batch(dataset2_x, dataset2_y, b)   \n",
    "    x1 = np.concatenate([matched1_x, random1_x])   \n",
    "    y1 = np.concatenate([matched1_y, random1_y])  \n",
    "    x2 = np.concatenate([matched2_x, random2_x])   \n",
    "    y2 = np.concatenate([matched2_y, random2_y])  \n",
    "    return x1, y1, x2, y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(embedded_batch_a, embedded_batch_b, a_labels, b_labels, margin):\n",
    "    y = tf.cast(tf.equal(a_labels, b_labels), tf.float32)\n",
    "    dist = tf.norm(embedded_batch_a - embedded_batch_b, axis=1)\n",
    "    #print(dist.shape)\n",
    "    loss = (y) * .5 * tf.square(dist) + (1-y) * .5 * tf.square(tf.maximum(0., margin - dist))\n",
    "    return y, tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(input_shape, labels_shape, \n",
    "                embed_size=128, learning_rate=0.001, \n",
    "                l1_reg=0.001, l2_reg=0.001, margin=50):\n",
    "    audio_inputs = tf.placeholder(shape=[None, input_shape[0], input_shape[1]], name=\"audio_inputs\", dtype=tf.float32)\n",
    "    video_inputs = tf.placeholder(shape=[None, input_shape[0], input_shape[1]], name=\"video_inputs\", dtype=tf.float32)\n",
    "    audio_labels = tf.placeholder(shape=[None], name=\"audio_labels\", dtype=tf.float32)\n",
    "    video_labels = tf.placeholder(shape=[None], name=\"video_labels\", dtype=tf.float32)\n",
    "\n",
    "    flattened_audio = tf.layers.flatten(audio_inputs)\n",
    "    flattened_video = tf.layers.flatten(video_inputs)\n",
    "    flattened_audio_labels = tf.layers.flatten(audio_labels)\n",
    "    flattened_video_labels = tf.layers.flatten(video_labels)\n",
    "\n",
    "    \n",
    "    #subnetwork = build_fc_net(inputs, embed_size, np.prod(video_input_shape), l1_reg, l2_reg)\n",
    "    weights = tf.trainable_variables()\n",
    "    reg_1 = tf.contrib.layers.l1_regularizer(scale=l1_reg)\n",
    "    reg_2 = tf.contrib.layers.l2_regularizer(scale=l2_reg)\n",
    "    audio_embed = build_fc_net(flattened_audio, embed_size, l1_reg, l2_reg)\n",
    "    video_embed = build_fc_net(flattened_video, embed_size, l1_reg, l2_reg, reuse=True)\n",
    "    weights = tf.trainable_variables()\n",
    "\n",
    "    #mse = tf.losses.mean_squared_error(audio_embed, video_embed) # only use loss if labels don't match?\n",
    "    matches, error = contrastive_loss(audio_embed, video_embed, flattened_audio_labels, flattened_video_labels, margin)\n",
    "    reg_penalty = tf.contrib.layers.apply_regularization(reg_1, weights) + tf.contrib.layers.apply_regularization(reg_2, weights)\n",
    "    loss = error + reg_penalty\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    return (audio_inputs, audio_labels,\n",
    "            video_inputs, video_labels,\n",
    "            loss, optimizer, matches,\n",
    "            audio_embed, video_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio input is a tf placeholder for the input audio features\n",
    "#video labels is a tf placeholder for the input video features\n",
    "#encode size is the desired size of the encoded vector (should be the same size as the video features)\n",
    "#l1 and l2 reg are the amount of weight to put on l1 and l2 regularizers for the loss\n",
    "\n",
    "def build_fc_net(input_data, embed_size, l1_reg=0.001, l2_reg=0.001, reuse=False):\n",
    "    net = build_fc_layers(input_data, embed_size, reuse)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer should be the flattened inputs\n",
    "def build_fc_layers(input_layer, output_size, reuse=False):\n",
    "    with tf.name_scope(\"model\"):\n",
    "        with tf.variable_scope(\"dense0\", reuse=reuse) as scope:\n",
    "            h1 = tf.layers.dense(inputs=input_layer, units=1024, activation=tf.nn.tanh)\n",
    "            d1 = tf.layers.dropout(inputs=h1, rate=.3)\n",
    "        with tf.variable_scope(\"dense1\", reuse=reuse) as scope:\n",
    "            h2 = tf.layers.dense(inputs=d1, units=1024, activation=tf.nn.tanh)\n",
    "            d2 = tf.layers.dropout(inputs=h2, rate=.3)            \n",
    "        with tf.variable_scope(\"dense2\", reuse=reuse) as scope:\n",
    "            h3 = tf.layers.dense(inputs=d2, units=1024, activation=tf.nn.tanh)\n",
    "            d3 = tf.layers.dropout(inputs=h3, rate=.3)\n",
    "        with tf.variable_scope(\"dense3\", reuse=reuse) as scope:\n",
    "            h4 = tf.layers.dense(inputs=d3, units=1024, activation=tf.nn.tanh)\n",
    "            d4 = tf.layers.dropout(inputs=h4, rate=.3)\n",
    "        with tf.variable_scope(\"dense4\", reuse=reuse) as scope:\n",
    "            h5 = tf.layers.dense(inputs=d3, units=1024, activation=tf.nn.tanh)\n",
    "            d5 = tf.layers.dropout(inputs=h4, rate=.3)\n",
    "        with tf.variable_scope(\"dense5\", reuse=reuse) as scope:\n",
    "            h6 = tf.layers.dense(inputs=d4, units=1024, activation=tf.nn.tanh)\n",
    "            d6 = tf.layers.dropout(inputs=h5, rate=.3)\n",
    "        with tf.variable_scope(\"dense6\", reuse=reuse) as scope:\n",
    "            raw_encode = tf.layers.dense(inputs=d3, units=output_size, activation=tf.nn.tanh)\n",
    "\n",
    "    return raw_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sess, audio_inputs, audio_labels, \n",
    "          video_inputs, video_labels, \n",
    "          audio_x, audio_y,\n",
    "          video_x, video_y,\n",
    "          loss, optimizer, \n",
    "          audio_embed, video_embed, \n",
    "          batch_size, cur_epoch, num_iters=1000):\n",
    "\n",
    "        for i in range(num_iters):\n",
    "            audio_batch_input, audio_batch_label, video_batch_input, video_batch_label = build_balanced_batch(audio_x, audio_y, video_x, video_y, batch_size)\n",
    "            \"\"\"if i % 2 == 0:\n",
    "                audio_batch_input, audio_batch_label, _ = build_batch(audio_x, audio_y, batch_size)\n",
    "                video_batch_input, video_batch_label, _ = build_batch(video_x, video_y, batch_size)\n",
    "            else:\n",
    "                audio_batch_input, audio_batch_label, indices = build_batch(audio_x, audio_y, batch_size)\n",
    "                video_batch_input, video_batch_label, _ = build_batch(video_x, video_y, batch_size, audio_batch_label)\"\"\"                \n",
    "            _, loss_val,ys = sess.run([optimizer, loss, matches], feed_dict={audio_inputs: audio_batch_input,\n",
    "                                                             video_inputs: video_batch_input,\n",
    "                                                             audio_labels: audio_batch_label,\n",
    "                                                             video_labels: video_batch_label})\n",
    "            if i % 100 == 0:\n",
    "                train_summary_writer.add_scalar('train_loss', loss_val, cur_epoch * num_iters + i)\n",
    "                #print(ys)\n",
    "#                 print(\"Loss at iter \" + str(i) + \": \" + str(loss_val))\n",
    "#                 print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbors(embedding, search_space, k):\n",
    "    embedding_stacked = np.repeat(embedding[np.newaxis,:], len(search_space), axis=0)\n",
    "    #print(embedding_stacked.shape)\n",
    "    similarity = np.linalg.norm(embedding_stacked - search_space, axis=1)\n",
    "    #print(similarity.shape)\n",
    "    indices = np.argsort(similarity)[:k]\n",
    "    #print(indices)\n",
    "    return indices\n",
    "\n",
    "def validate(sess, audio_inputs, audio_labels, \n",
    "             video_inputs, video_labels, \n",
    "             audio_x, audio_y,\n",
    "             video_x, video_y,\n",
    "             loss, optimizer, \n",
    "             audio_embed, video_embed, \n",
    "             cur_epoch, batch_size=100, k=50, tfboard_var='val_found_matches'):\n",
    "\n",
    "    audio_batch_input, audio_batch_label, indices = build_batch(audio_x, audio_y, batch_size)\n",
    "    #print(audio_batch_label)\n",
    "    video_batch_input, video_batch_label, _ = build_batch(video_x, video_y, batch_size, audio_batch_label)\n",
    "    #print(video_batch_label)\n",
    "    audio_embeddings = sess.run(audio_embed, feed_dict={audio_inputs: audio_batch_input,\n",
    "                                                          audio_labels: audio_batch_label})\n",
    "    video_embeddings = sess.run(video_embed, feed_dict={video_inputs: video_batch_input,\n",
    "                                                          video_labels: video_batch_label})\n",
    "    matches = 0\n",
    "    for i in range(len(audio_embeddings)):\n",
    "        a = audio_embeddings[i]\n",
    "        #if i%10 == 0:\n",
    "        #    print(a)\n",
    "        a_y = audio_batch_label[i]\n",
    "        ids = k_nearest_neighbors(a, video_embeddings, k)\n",
    "#         print(a_y)\n",
    "#         print(ids)\n",
    "#         print(video_batch_label)\n",
    "#         print(video_batch_label[ids])\n",
    "        if a_y in video_batch_label[ids]:\n",
    "            matches += 1\n",
    "#     print(\"\")\n",
    "#     print(\"Percent of matches found: \" + str(matches / len(audio_embeddings)))\n",
    "    train_summary_writer.add_scalar(tfboard_var, matches/batch_size, cur_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log_dir = 'logs/tensorboard/train/log9'\n",
    "train_summary_writer = SummaryWriter(train_log_dir)\n",
    "\n",
    "\n",
    "num_epochs = 500\n",
    "batch = 64\n",
    "embed_size = 128\n",
    "learning_rate = 0.0001\n",
    "l1_reg = 0.001\n",
    "l2_reg = 0.001\n",
    "margin=2\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "(audio_inputs, audio_labels, \n",
    " video_inputs, video_labels, \n",
    " loss, optimizer, matches,\n",
    " audio_embed, video_embed) = build_graph(audio_x[0].shape, audio_y.shape[0], embed_size, learning_rate, l1_reg, l2_reg, margin)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "#     print(\"\")\n",
    "#     print(\"Epoch \" + str(epoch))\n",
    "#     print(\"=============\")\n",
    "#     print(\"\")\n",
    "    train(sess, audio_inputs, audio_labels,\n",
    "          video_inputs, video_labels,\n",
    "          training_audio_x, training_audio_y, \n",
    "          training_video_x, training_video_y, \n",
    "          loss, optimizer, \n",
    "          audio_embed, video_embed, batch, epoch)\n",
    "    validate(sess, audio_inputs, audio_labels,\n",
    "          video_inputs, video_labels,\n",
    "          training_audio_x, training_audio_y, \n",
    "          training_video_x, training_video_y, \n",
    "          loss, optimizer, \n",
    "          audio_embed, video_embed, epoch, batch_size=validation_split, tfboard_var='train_found_matches')\n",
    "\n",
    "    validate(sess, audio_inputs, audio_labels,\n",
    "          video_inputs, video_labels,\n",
    "          validation_audio_x, validation_audio_y, \n",
    "          validation_video_x, validation_video_y, \n",
    "          loss, optimizer, \n",
    "          audio_embed, video_embed, epoch, batch_size=validation_split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(sess, audio_inputs, audio_labels,\n",
    "      video_inputs, video_labels,\n",
    "      audio_x, audio_y, \n",
    "      video_x, video_y, \n",
    "      loss, optimizer, \n",
    "      audio_embed, video_embed, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
