{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from youtube-8m utils.py\n",
    "def Dequantize(feat_vector, max_quantized_value=2, min_quantized_value=-2):\n",
    "  \"\"\"Dequantize the feature from the byte format to the float format.\n",
    "\n",
    "  Args:\n",
    "    feat_vector: the input 1-d vector.\n",
    "    max_quantized_value: the maximum of the quantized value.\n",
    "    min_quantized_value: the minimum of the quantized value.\n",
    "\n",
    "  Returns:\n",
    "    A float vector which has the same shape as feat_vector.\n",
    "  \"\"\"\n",
    "  assert max_quantized_value > min_quantized_value\n",
    "  quantized_range = max_quantized_value - min_quantized_value\n",
    "  scalar = quantized_range / 255.0\n",
    "  bias = (quantized_range / 512.0) + min_quantized_value\n",
    "  return feat_vector * scalar + bias\n",
    "\n",
    "def decode(feat_vector, feature_size):\n",
    "    return tf.reshape(tf.cast(tf.decode_raw(feat_vector, \n",
    "                                            tf.uint8), \n",
    "                              tf.float32),\n",
    "                      [-1, feature_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath is path to tfrecord\n",
    "# datatype is audio or video\n",
    "# output_features and output_labels are empty lists or existing lists\n",
    "def load_data(filepath, data_type, output_labels, output_features):\n",
    "    if data_type == 'audio':\n",
    "        context = {\n",
    "            'labels': tf.VarLenFeature(dtype=tf.int64)\n",
    "        }\n",
    "\n",
    "        feature_list = {\n",
    "            'audio_embedding': tf.FixedLenSequenceFeature([], dtype=tf.string)\n",
    "        }\n",
    "        feature_name = 'audio_embedding'\n",
    "        feature_len = 128\n",
    "\n",
    "    elif data_type == 'video':\n",
    "        context = {\n",
    "            'id': tf.FixedLenFeature([], dtype=tf.string),\n",
    "            'labels': tf.VarLenFeature(dtype=tf.int64)\n",
    "        }\n",
    "\n",
    "        feature_list = {\n",
    "            'rgb': tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "        }\n",
    "        feature_name = 'rgb'\n",
    "        feature_len = 128\n",
    "        \n",
    "        \n",
    "    tf.reset_default_graph()    \n",
    "    sess = tf.InteractiveSession()\n",
    "\n",
    "    # Read TFRecord file\n",
    "    reader = tf.TFRecordReader()\n",
    "    filename_queue = tf.train.string_input_producer([filepath])\n",
    "\n",
    "\n",
    "    # Extract features from serialized data\n",
    "\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    context, features = tf.io.parse_single_sequence_example(serialized_example,\n",
    "                                                    context_features=context,\n",
    "                                                    sequence_features=feature_list,\n",
    "                                                    example_name=None,\n",
    "                                                    name=None\n",
    "    )\n",
    "\n",
    "    # Many tf.train functions use tf.train.QueueRunner,\n",
    "    # so we need to start it before we read\n",
    "    tf.train.start_queue_runners(sess)\n",
    "    \n",
    "    \n",
    "    num_in_file = sum(1 for _ in tf.python_io.tf_record_iterator(filepath))\n",
    "\n",
    "    for i in range(num_in_file):\n",
    "        labels = context['labels'].eval()\n",
    "        label = labels.values[0]\n",
    "        data = Dequantize(decode(features[feature_name], feature_len)).eval()\n",
    "        output_labels.append(label)\n",
    "        output_features.append(data)\n",
    "\n",
    "    tf.InteractiveSession().close()\n",
    "    \n",
    "    return output_labels, output_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "audio_output_labels = []\n",
    "audio_output_features = []\n",
    "audio_path = \"audio_1556745450.370243.tfrecord\"\n",
    "audio_output_labels, audio_output_features = load_data(audio_path,\n",
    "                                           'audio', audio_output_labels, audio_output_features)\n",
    "\n",
    "video_output_labels = []\n",
    "video_output_features = []\n",
    "video_path = \"video_1556754626.438139.tfrecord\"\n",
    "video_output_labels, video_output_features = load_data(video_path,\n",
    "                                           'video', video_output_labels, video_output_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_x shape:  (450, 10, 128)\n",
      "audio_y shape:  (450,)\n",
      "video_x shape:  (450, 10, 128)\n",
      "video_y shape:  (450,)\n"
     ]
    }
   ],
   "source": [
    "audio_x = np.array(audio_output_features)\n",
    "audio_y = np.array(audio_output_labels)\n",
    "video_x = np.array(video_output_features)\n",
    "video_y = np.array(video_output_labels)\n",
    "print('audio_x shape: ', audio_x.shape)\n",
    "print('audio_y shape: ', audio_y.shape)\n",
    "print('video_x shape: ', video_x.shape)\n",
    "print('video_y shape: ', video_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- At this point the desired data should be loaded --- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batch(dataset, batch_size):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_model_1(features, labels, mode):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(\n",
    "      inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Logits Layer\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize(\n",
    "        loss=loss,\n",
    "        global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input layer is a tf placeholder that is the input data\n",
    "#output size is the desired size of the encoded vector (should be the same size as the video vector?)\n",
    "#scope is a string to keep track of all trainable variables in this stack\n",
    "def build_fc_layers(input_layer, output_size, scope):\n",
    "    h1 = tf.layers.dense(inputs=input_layer, units=1024, activation=tf.nn.tanh, scope=scope)\n",
    "    d1 = tf.layers.dropout(inputs=h1, rate=0.4)\n",
    "    h2 = tf.layers.dense(inputs=d1, units=1024, activation=tf.nn.tanh, scope=scope)\n",
    "    d2 = tf.layers.dropout(inputs=h2, rate=0.4)\n",
    "    h3 = tf.layers.dense(inputs=d2, units=1024, activation=tf.nn.tanh, scope=scope)\n",
    "    d3 = tf.layers.drouput(inputs=h3, rate=0.4)\n",
    "    raw_encode = tf.layers.dense(inputs=d3, units=output_size, activation=tf.nn.tanh, scope=scope)\n",
    "    return raw_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#audio input is a tf placeholder for the input audio features\n",
    "#video labels is a tf placeholder for the input video features\n",
    "#encode size is the desired size of the encoded vector (should be the same size as the video features)\n",
    "#l1 and l2 reg are the amount of weight to put on l1 and l2 regularizers for the loss\n",
    "def build_fc_net(audio_input, video_labels, encode_size, l1_reg=0, l2_reg=0):\n",
    "    audio_net = build_fc_layers(audio_input, encode_size, scope=\"fc_audio\")\n",
    "    weights = tf.trainable_variables(scope=\"fc_audio\")\n",
    "    if l1_reg != 0:\n",
    "        reg_1 = tf.contrib.layers.l1_regularizer(scale=l1_reg)\n",
    "    else:\n",
    "        reg_1 = 0\n",
    "    if l2_reg != 0:\n",
    "        reg_2 = tf.contrib.layers.l2_regularizer(scale=l2_reg)\n",
    "    else:\n",
    "        reg_2 = 0\n",
    "    mse = tf.losses.mean_squared_error(audio_net, video_labels)\n",
    "    reg_penalty = tf.contrib.layers.apply_regularization(reg_1, weights) + tf.contrib.layers.apply_regularization(reg_2, weights)\n",
    "    loss = mse + reg_penalty\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(audio_input_shape, video_input_shape, learning_rate=0.0001, l1_reg=0, l2_reg=0):\n",
    "    inputs = tf.placeholder(shape=audio_input_shape, name=\"inputs\", dtype=tf.float32)\n",
    "    labels = tf.placeholder(shape=video_input_shape, name=\"labels\", dtype=tf.float32)\n",
    "    flattened_labels = tf.layers.Flatten()(labels)\n",
    "    loss = build_fc_net(inputs, flattened_labels, np.prod(video_input_shape), l1_reg, l2_reg)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    return inputs, labels, loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inputs, labels, loss, optimizer, dataset, batch_size, num_iters=1000):\n",
    "    with tf.Session() as sess:\n",
    "        for i in range(num_iters):\n",
    "            batch_input, batch_label = build_batch(dataset, batch_size)\n",
    "            _, loss = sess.run([optimizer, loss], feed_dict={inputs:batch_input, labels:batch_label})\n",
    "            if i % 100 == 0:\n",
    "                print(\"Loss at batch \" + str(i))\n",
    "                print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
